

\documentclass[paper=a4, fontsize=11pt]{scrartcl} % A4 paper and 11pt font size

\usepackage[T1]{fontenc} % Use 8-bit encoding that has 256 glyphs
\usepackage{fourier} % Use the Adobe Utopia font for the document - comment this line to return to the LaTeX default
\usepackage[english]{babel} % English language/hyphenation
\usepackage{amsmath,amsfonts,amsthm} % Math packages

\usepackage{lipsum} % Used for inserting dummy 'Lorem ipsum' text into the template

\usepackage{sectsty} % Allows customizing section commands
\allsectionsfont{\centering \normalfont\scshape} % Make all sections centered, the default font and small caps

\usepackage{fancyhdr} % Custom headers and footers
\pagestyle{fancyplain} % Makes all pages in the document conform to the custom headers and footers
\fancyhead[C]{Big Data} % No page header - if you want one, create it in the same way as the footers below
\fancyfoot[L]{University of New Mexico} % Empty left footer
\fancyfoot[C]{} % Empty center footer
\fancyfoot[R]{\thepage} % Page numbering for right footer
\renewcommand{\headrulewidth}{0pt} % Remove header underlines
\renewcommand{\footrulewidth}{0pt} % Remove footer underlines
\setlength{\headheight}{13.6pt} % Customize the height of the header

\numberwithin{equation}{section} % Number equations within sections (i.e. 1.1, 1.2, 2.1, 2.2 instead of 1, 2, 3, 4)
\numberwithin{figure}{section} % Number figures within sections (i.e. 1.1, 1.2, 2.1, 2.2 instead of 1, 2, 3, 4)
\numberwithin{table}{section} % Number tables within sections (i.e. 1.1, 1.2, 2.1, 2.2 instead of 1, 2, 3, 4)

\setlength\parindent{0pt} % Removes all indentation from paragraphs - comment this line for an assignment with lots of text

\newcommand{\horrule}[1]{\rule{\linewidth}{#1}} % Create horizontal rule command with 1 argument of height

\title{	
\normalfont \normalsize 
\textsc{University of New Mexico, Big Data} \\ [25pt] % Your university, school and/or department name(s)
\horrule{0.5pt} \\[0.4cm] % Thin top horizontal rule
\huge Big Data Project Proposal \\ % The assignment title
\horrule{2pt} \\[0.5cm] % Thick bottom horizontal rule
}

\author{Cody W. Eilar\\ 
Venkatesh Jatla
} 

\date{\normalsize\today} 

\begin{document}

\maketitle
\thispagestyle{empty}
\section{Introduction}
The advancing out of school learning in mathematics and engineering (AOLME) is program designed to help under 
represented middle school students to become interested in science, technology, engineering and mathematics (STEM). The goal of
the program is to not only provide these students with access to high quality programs, but to also help educators analyze
what learning methods work well in the classroom. Using video cameras in the classroom, AOLME attempts to capture 
thousands of hours of student to student and student to facilitator interactions. The goal is then to analyze these videos
to further help educators understand what is working in the classroom and what is not, i.e. when students are learning or when they are not. The problem with this currently is that
all the videos must be annotated by hand which is very time consuming and tedious. To aid educators with their research, 
it would be extremely useful to have automated methods for annotating videos autonomously using machine learning and 
video processing techniques. Because AOLME has so many videos, it seems that "big data" techniques may also be a
good fit for solving the problem of autonomous video annotation. In the following sections, I will outline how I think
"big data" techniques can be applied to solving this difficult problem. 

\section{Proposal}
The problem that AOLME is trying to solve is very difficult at an engineering level. There have been several published
papers on action recognition using support vector machines (SVMs) in combination with motion vectors, but there is still
much research to be done in this area. The datasets that are provided by AOLME are perfect for this type of research because
of the large number of videos that are available. Providing access to these videos though is not simple because of their 
sensitivity and thus an IRB must be completed before any researcher may use the videos. However, if the videos had blurred 
faces they could be used more freely. For this reason, I believe AOLME would benefit from a program that could systematically 
process the terabytes of video data and create a "clean" dataset that can more easily distributed amongst researchers by 
blurring the faces of the students. The vision is described further in the following sections. 

\subsection{Terabytes of Videos} 
AOLME has collected many gigabytes of videos, and they plan on collecting many more. As a result, there will be petabytes
of data to handle. Though the data doesn't seem to be the typical fit of \textit{text} driven examples that we have been
looking at in class, I believe that many of the same concepts of \textit{map-reduce} can still play a role in making video
analysis manageable and distributable over many machines over network.\\
Distributing videos over network brings up another problem. Videos are inherently large, using latest compression 
techniques, like HEVC (High Efficiency Video Coding), to \textit{transcode} will help in reducing the bandwidth.

\subsection{Machine Learning Aspects}
Face recognition is already a well established science in the machine learning world. Because it is already well established, 
I believe that it will be a good place to start to eventually arrive at action detection. Time permitting, I would also like to 
explore some \textit{random forest} and \textit{support vector machines} to further enhance the algorithms. There are
probably many more avenues to explore in this area, but for the sake of this class I would like to keep it fairly brief. 

\subsection{Caveats}
There is one caveat with this project and that is that an IRB must be completed in order to view the current datasets.  
There is the possibility that we could work with other datasets  to "prove our point", but it would be much more useful 
if we could work with the real data to provide useful results. 

\end{document}



